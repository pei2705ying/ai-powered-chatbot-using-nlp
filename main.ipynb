{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76662194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\peiying\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\peiying\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\peiying\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\peiying\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\peiying\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import datetime as dt\n",
    "import random\n",
    "import re, collections  \n",
    "\n",
    "nltk.download()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b7d165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "def token_stemming(tokens):\n",
    "    new_tokens = []\n",
    "    sb_stemmer = SnowballStemmer('english') # using snowball stemmer\n",
    "    for token in tokens:\n",
    "        new_tokens.append(sb_stemmer.stem(token))\n",
    "    return new_tokens\n",
    "\n",
    "def token_lemmatisation(tokens):\n",
    "    new_tokens = []\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    posmap = {\n",
    "        'ADJ': 'a',\n",
    "        'ADV': 'r',\n",
    "        'NOUN': 'n',\n",
    "        'VERB': 'v'\n",
    "    }\n",
    "    # process the lemmatisation with tags\n",
    "    post = nltk.pos_tag(tokens, tagset='universal') \n",
    "    for token in post:\n",
    "        word, tag = token[0], token[1]\n",
    "        if tag in posmap.keys():\n",
    "            new_tokens.append(lemmatiser.lemmatize(word, posmap[tag]))\n",
    "        else:\n",
    "            new_tokens.append(lemmatiser.lemmatize(word))\n",
    "    return new_tokens\n",
    "\n",
    "def text_preprocessing(text, type):\n",
    "    # tokenise\n",
    "    text_tokens = word_tokenize(text)\n",
    "    # remove stop words and special signs \n",
    "    tokens = [word.lower() for word in text_tokens if not word in stopwords.words('english') and word.isalpha()]\n",
    "    # stemming or lemmatisation\n",
    "    tokens = token_lemmatisation(tokens) if type == 'lemmatisation' else token_stemming(tokens)\n",
    "    return (' ').join(tokens)\n",
    "\n",
    "def time_response(str):\n",
    "    date = dt.datetime.now()\n",
    "    if str == 'time':\n",
    "        hour = date.strftime(\"%H\")\n",
    "        minute = date.strftime(\"%M\") \n",
    "        second = date.strftime(\"%S\")\n",
    "        print(\">> Naevis: Do you mean the current time? It's %s:%s:%s now. ‚è∞\" %(hour,minute,second))\n",
    "        if int(hour) < 6:\n",
    "            print(\">> Naevis: Oh, don't wake me up! ü•±\" )\n",
    "        elif int(hour) < 11:\n",
    "            print(\">> Naevis: Morning! üåû\")\n",
    "        elif int(hour) < 13:\n",
    "            print(\">> Naevis: I am eating lunch now! üòù\")\n",
    "        elif int(hour) < 18:\n",
    "            print(\">> Naevis: I am busy studying ü§Ø\")\n",
    "        elif int(hour) < 20:\n",
    "            print(\">> Naevis: I am having dinner now! üòã\")\n",
    "        else:\n",
    "            print(\">> Naevis: I need charge now! Good night! üò¥\")\n",
    "        \n",
    "    else:\n",
    "        year = date.year\n",
    "        month = date.month  \n",
    "        day = date.day\n",
    "        print(\">> Jarvis: Today is Day %s/ Month %s/ Year %s! üìÜ\" %(day, month, year))\n",
    "\n",
    "      \n",
    "def emotion():\n",
    "    e_list = ['üòÅ','üòâ','üòä','‚ò∫','üòÜ',\n",
    "              'ü§î','üòó','ü§ó ','ü§≠','üßê',\n",
    "              'ü§ì','ü•≥','üòÄ','üòÉ','üòÑ',\n",
    "              'üôÇ','üòã','üòé','üòç','ü•∞',\n",
    "              'ü§©','üòö','üòô','üòõ','üòú',\n",
    "              'üòØ','üò≤','üò¨','üòá ','üôà'\n",
    "    ]\n",
    "    id = random.randint(0, len(e_list)-1)\n",
    "    return e_list[id]\n",
    " \n",
    "import re, collections\n",
    "\n",
    "def words(text): return re.findall('[a-z]+', text.lower()) \n",
    "\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "\n",
    "NWORDS = train(words(open(\"text_check.txt\", \"r\").read()))\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def edits1(word):\n",
    "   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "   deletes    = [a + b[1:] for a, b in splits if b]\n",
    "   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "   inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
    "   return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "\n",
    "def known(words): return set(w for w in words if w in NWORDS)\n",
    "\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "    return max(candidates, key=NWORDS.get)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # emoji = emotion()\n",
    "    # print(emoji)\n",
    "    # time_response('time')\n",
    "    print(correct('helllo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52619a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_Q(query, threshold):\n",
    "\n",
    "    df = pd.read_csv('question_answer.csv')\n",
    "    df['processed_Q'] = df['Question'].apply(text_preprocessing, type = 'stemming')\n",
    "\n",
    "    # TF-IDF\n",
    "    tfidf_vec = TfidfVectorizer(analyzer='word')\n",
    "    X_tfidf = tfidf_vec.fit_transform(df['Question']).toarray()\n",
    "    df_tfidf = pd.DataFrame(X_tfidf, columns = tfidf_vec.get_feature_names_out())\n",
    "\n",
    "    # process query and find the answer\n",
    "    processed_query = text_preprocessing(query, 'stemming')\n",
    "    input_tfidf = tfidf_vec.transform([processed_query]).toarray()\n",
    "    cos = 1 - pairwise_distances(df_tfidf, input_tfidf, metric = 'cosine')\n",
    "    \n",
    "    if cos.max() >= threshold:\n",
    "        id_argmax = np.where(cos == np.max(cos, axis=0))\n",
    "        id = np.random.choice(id_argmax[0]) \n",
    "        return df['Answer'].loc[id]\n",
    "    else:\n",
    "        return 'NOT FOUND'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89091fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANGE_NAME = [\"switch\", \"change\", \"call\"]\n",
    "NAME = [\"call\", \"me\", \"change\", \"my\", \"name\", \"to\", \"please\", \"switch\", \"yes\", \"sure\"]\n",
    "\n",
    "def check_name_change(input):\n",
    "    text_tokens = word_tokenize(input)\n",
    "    if not set(text_tokens).isdisjoint(CHANGE_NAME):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def name_change(input):\n",
    "    text_tokens = word_tokenize(input)\n",
    "    user_name = [i for i in text_tokens if not i.lower() in NAME and i.isalpha() and not i.lower() in stopwords.words('english')]\n",
    "    user_name = (' ').join(user_name)\n",
    "    return user_name\n",
    "\n",
    "# path of small talk dataset\n",
    "# data_path = 'COMP3074-CW1-Dataset-name.csv'\n",
    "\n",
    "def name_response(query, threshold):\n",
    "\n",
    "    df = pd.read_csv('name_management.csv')\n",
    "\n",
    "    # TF-IDF\n",
    "    tfidf_vec = TfidfVectorizer(analyzer='word')\n",
    "    X_tfidf = tfidf_vec.fit_transform(df['Question']).toarray()\n",
    "    df_tfidf = pd.DataFrame(X_tfidf, columns = tfidf_vec.get_feature_names_out())\n",
    "\n",
    "    # process query \n",
    "    input_tfidf = tfidf_vec.transform([query.lower()]).toarray()\n",
    "\n",
    "    # cosine similarity\n",
    "    cos = 1 - pairwise_distances(df_tfidf, input_tfidf, metric = 'cosine')\n",
    "    \n",
    "    if cos.max() >= threshold:\n",
    "        return 'RESPOND'\n",
    "    else:\n",
    "        return 'NOT FOUND'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a849b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not much. how about you?\n"
     ]
    }
   ],
   "source": [
    "# path of small talk dataset\n",
    "\n",
    "def talk_response(query, threshold):\n",
    "\n",
    "    df = pd.read_csv('small_talk.csv')\n",
    "\n",
    "    # TF-IDF\n",
    "    tfidf_vec = TfidfVectorizer(analyzer='word')\n",
    "    X_tfidf = tfidf_vec.fit_transform(df['Question']).toarray()\n",
    "    df_tfidf = pd.DataFrame(X_tfidf, columns = tfidf_vec.get_feature_names_out())\n",
    "\n",
    "    # process query\n",
    "    processed_query = text_preprocessing(query, 'stemming')\n",
    "    input_tfidf = tfidf_vec.transform([query.lower()]).toarray()\n",
    "\n",
    "    # cosine similarity\n",
    "    cos = 1 - pairwise_distances(df_tfidf, input_tfidf, metric = 'cosine')\n",
    "    \n",
    "    if cos.max() >= threshold:\n",
    "        id_argmax = np.where(cos == np.max(cos, axis=0))\n",
    "        id = np.random.choice(id_argmax[0]) \n",
    "        return df['Answer'].loc[id]\n",
    "    else:\n",
    "        return 'NOT FOUND'\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print(talk_response(\"What is up\", 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9613ba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Naevis: Hey there! I'm Naevis ü§ñ nice to meet you.\n",
      "           Please Enter 'bye' if you wish to quit.\n",
      "           May I have ur name? üò¨\n",
      ">> user:  pei\n",
      ">> Naevis: Hi, pei, glad to know u! ü§©\n",
      ">> pei:  change name to p\n",
      ">> Naevis: Hi, p! ü§ó \n",
      ">> p:  how is everything going?\n",
      ">> Naevis:  i am great. Thanks üôà\n",
      ">> p:  what is the date today?\n",
      ">> Jarvis: Today is Day 9/ Month 12/ Year 2022! üìÜ\n",
      ">> p:  how about the time now?\n",
      ">> Naevis: Do you mean the current time? It's 02:38:22 now. ‚è∞\n",
      ">> Naevis: Oh, don't wake me up! ü•±\n",
      ">> p:  who am i?\n",
      ">> Naevis: I have a good memory. YOU ARE p ü§î\n",
      ">> p:  what's up?\n",
      ">> Naevis: not much. how about you? üòç\n",
      ">> p:  i'm fine\n",
      ">> Naevis:  glad to know you're fine. do u need any help? üòç\n",
      ">> p:  what is white chocolate made of?\n",
      ">> Naevis: It commonly consists of cocoa butter , sugar , milk solids and salt , and is characterized by a pale yellow or ivory appearance. üòÉ\n",
      ">> p:  what is linked in?\n",
      ">> Naevis: I'm sorry. I don't understand. Please rephrase your question. üòî\n",
      ">> p:  what is linkedin?\n",
      ">> Naevis: LinkedIn () is a social networking website for people in professional occupations. ü§ó \n",
      ">> p:  tell me a joke\n",
      ">> Naevis: which is faster, hot or cold? Hot, because you can catch cold. üòç\n",
      ">> p:  "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    user_name = 'user'\n",
    "\n",
    "    flag = True\n",
    "    print(\">> Naevis: Hey there! I'm Naevis ü§ñ nice to meet you.\")\n",
    "    print(\"           Please Enter 'bye' if you wish to quit.\")\n",
    "    print(\"           May I have ur name? %s\" %emotion())\n",
    "    print('>> %s: ' %user_name, end=\" \")\n",
    "    user_input = input()\n",
    "    if user_input == 'bye':\n",
    "        flag = False\n",
    "    else:\n",
    "        user_name = name_change(user_input)\n",
    "        if user_name.lower() == 'lee soo man':\n",
    "            print(\">> Naevis: Oh, hi boss! SM Town Naevis' here to serve you! %s\" %emotion())\n",
    "        else:\n",
    "            print(\">> Naevis: Hi, %s, glad to know u! %s\" %(user_name, emotion()))\n",
    "\n",
    "    while(flag == True):\n",
    "        print('>> %s: '%user_name, end=\" \")\n",
    "        user_input = input()\n",
    "        user_input = user_input.lower()\n",
    "        user_input = [correct(i) for i in user_input.split(' ')]\n",
    "        user_input = (' ').join(user_input)\n",
    "        if(user_input != 'bye'):\n",
    "\n",
    "            # name management\n",
    "            response = name_response(user_input, threshold = 0.9)\n",
    "            if response != 'NOT FOUND':\n",
    "                print(\">> Naevis: I have a good memory. YOU ARE %s %s\" %(user_name,emotion()))\n",
    "                continue\n",
    "\n",
    "            if check_name_change(user_input):\n",
    "                user_name = name_change(user_input)\n",
    "                print(\">> Naevis: Hi, %s! %s\" %(user_name, emotion()))\n",
    "                continue\n",
    "            \n",
    "            # time and data -- a part of the small talk\n",
    "            if 'time' in user_input:\n",
    "                time_response('time')\n",
    "                continue\n",
    "\n",
    "            if  'today' in user_input:\n",
    "                time_response('today')\n",
    "                continue\n",
    "\n",
    "            # small talk\n",
    "            response = talk_response(user_input, threshold = 0.7)\n",
    "            if response != 'NOT FOUND':\n",
    "                print(\">> Naevis: \" + response + ' ' + emotion())\n",
    "                continue\n",
    "\n",
    "            # Question Answering\n",
    "            response = answer_Q(user_input, threshold = 0.1)\n",
    "            if response != 'NOT FOUND':\n",
    "                print(\">> Naevis: \" + response + ' ' + emotion())\n",
    "            else:\n",
    "                print(\">> Naevis: I'm sorry. I don't understand. Please rephrase your question. üòî\")\n",
    "\n",
    "        else:\n",
    "            flag = False\n",
    "    print(\">> Naevis: Bye! Take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ca518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
